{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63b553ac-7b27-40f1-9e51-103872d0f84e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading and cleaning chunks: 100%|██████████████| 11/11 [00:19<00:00,  1.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Finished loading. Shape: (5383378, 30)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ============================================================\n",
    "# 🔹 ENABLE TQDM PROGRESS BARS FOR PANDAS\n",
    "# ============================================================\n",
    "tqdm.pandas()  # adds progress_apply to pandas\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1️) LOAD DATA (with progress tracking & efficient cleaning)\n",
    "# ============================================================\n",
    "\n",
    "# Initialize tqdm progress bar for chunks\n",
    "chunksize = 500_000\n",
    "filename = \"All Recorded Traffic.txt\"\n",
    "\n",
    "# Count total lines first (optional, but lets tqdm show % complete)\n",
    "with open(filename, 'r') as f:\n",
    "    total_lines = sum(1 for _ in f)\n",
    "total_chunks = total_lines // chunksize + 1\n",
    "\n",
    "parts = []\n",
    "\n",
    "# Read in chunks with progress bar\n",
    "for ch in tqdm(pd.read_csv(filename, sep=\"\\t\", dtype=str, chunksize=chunksize),\n",
    "               total=total_chunks, desc=\"Reading and cleaning chunks\"):\n",
    "    # Clean column names\n",
    "    ch.columns = ch.columns.str.strip()\n",
    "\n",
    "    # Strip whitespace only in string columns\n",
    "    str_cols = ch.select_dtypes(include=[\"object\"]).columns\n",
    "    ch[str_cols] = ch[str_cols].apply(lambda col: col.str.strip())\n",
    "\n",
    "    parts.append(ch)\n",
    "\n",
    "# Combine all chunks into one dataframe\n",
    "traffic = pd.concat(parts, ignore_index=True)\n",
    "\n",
    "print(f\"\\n Finished loading. Shape: {traffic.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5318555d-1810-4171-bb8b-7c8d7501e010",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Normalizing TIME column...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████| 5383378/5383378 [00:15<00:00, 341588.60it/s]\n",
      "100%|███████████████████████████████| 5383378/5383378 [12:26<00:00, 7208.84it/s]\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 2️) NORMALIZE DATE / TIME\n",
    "# ============================================================\n",
    "\n",
    "traffic['DATE'] = pd.to_datetime(traffic['DATE'], errors='coerce')\n",
    "\n",
    "def normalize_time(t):\n",
    "    \"\"\"Standardize various time formats into consistent HH:MM:SS.\"\"\"\n",
    "    if pd.isna(t):\n",
    "        return pd.NaT\n",
    "    s = str(t).strip()\n",
    "    if ':' in s:\n",
    "        s = s.split('.')[0]\n",
    "        for fmt in ['%H:%M:%S', '%H:%M']:\n",
    "            try:\n",
    "                return pd.to_datetime(s, format=fmt).time()\n",
    "            except:\n",
    "                continue\n",
    "        return pd.NaT\n",
    "    digits = ''.join(ch for ch in s if ch.isdigit()).zfill(4)[-4:]\n",
    "    try:\n",
    "        return datetime.strptime(digits, '%H%M').time()\n",
    "    except:\n",
    "        return pd.NaT\n",
    "\n",
    "print(\"\\n Normalizing TIME column...\")\n",
    "traffic['TIME_parsed'] = traffic['TIME'].progress_apply(normalize_time)\n",
    "\n",
    "traffic['DATETIME'] = traffic.progress_apply(\n",
    "    lambda r: pd.NaT if pd.isna(r['DATE']) or pd.isna(r['TIME_parsed'])\n",
    "    else pd.to_datetime(f\"{r['DATE'].date()} {r['TIME_parsed'].strftime('%H:%M:%S')}\"),\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d9929a8-faba-4634-8f12-54db58da6e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Converting numeric columns...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Numeric conversion: 100%|█████████████████████████| 8/8 [00:11<00:00,  1.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric columns converted\n",
      "\n",
      "Converting CLASS columns and filling NaNs with 0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CLASS numeric conversion: 100%|█████████████████| 10/10 [00:10<00:00,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASS columns converted and NaNs filled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 3️) NUMERIC CONVERSION\n",
    "# ============================================================\n",
    "\n",
    "num_cols = ['TOTAL', 'CASH', 'EZPASS', 'VIOLATION', 'Autos', 'Small_T', 'Large_T', 'Buses']\n",
    "\n",
    "print(\"\\n Converting numeric columns...\")\n",
    "for c in tqdm(num_cols, desc=\"Numeric conversion\"):\n",
    "    if c in traffic.columns:\n",
    "        traffic[c] = pd.to_numeric(traffic[c].replace(['NULL', ''], np.nan), errors='coerce')\n",
    "print(\"Numeric columns converted\")\n",
    "\n",
    "# --- Convert CLASS columns to numeric to avoid Parquet errors. And Fill nulls in CLASS columns with 0\n",
    "class_cols = [c for c in traffic.columns if c.upper().startswith('CLASS')]\n",
    "print(\"\\nConverting CLASS columns and filling NaNs with 0...\")\n",
    "for c in tqdm(class_cols, desc=\"CLASS numeric conversion\"):\n",
    "    traffic[c] = pd.to_numeric(traffic[c], errors='coerce').fillna(0).astype(float)\n",
    "print(\"CLASS columns converted and NaNs filled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1950e8a-a3b4-488e-a9d8-c8a274414afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔄 Calculating class sums...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summing CLASS columns: 100%|████████████████████| 10/10 [00:00<00:00, 61.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class mismatch rows: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 4️) CLASS SUM CHECK (with progress)\n",
    "# ============================================================\n",
    "\n",
    "class_cols = [c for c in traffic.columns if c.upper().startswith('CLASS')]\n",
    "\n",
    "if class_cols:\n",
    "    print(\"\\n🔄 Calculating class sums...\")\n",
    "    traffic['class_sum'] = 0\n",
    "    for c in tqdm(class_cols, desc=\"Summing CLASS columns\"):\n",
    "        traffic['class_sum'] += traffic[c].fillna(0).astype(float)\n",
    "    traffic['class_mismatch'] = (traffic['TOTAL'] - traffic['class_sum']).abs() > 0.01\n",
    "    mismatch_count = traffic['class_mismatch'].sum()\n",
    "    print(f\"Class mismatch rows: {mismatch_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "700d55fc-30a5-4af0-95de-a12a3904f3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Drop redundant columns\n",
    "traffic = traffic.drop(columns=['Month', 'Yr', 'FAC_G2'], errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecfa293-c30a-425b-96b7-1a00053edc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 5️) STRATIFIED SAMPLING (5%) — progress optional\n",
    "# ============================================================\n",
    "\n",
    "#traffic['month_year'] = traffic['DATE'].dt.to_period('M')\n",
    "\n",
    "#print(\"\\n Creating stratified 5% sample (safe for all pandas versions)...\")\n",
    "#sample = (\n",
    "#    traffic.groupby(['FAC_B', 'month_year'], group_keys=False)\n",
    "#           .progress_apply(lambda x: x.sample(frac=0.05, random_state=1) if len(x) > 1 else x)\n",
    "#           .reset_index(drop=True)  # ensures no duplicate index issues\n",
    "#)\n",
    "#print(f\"Stratified sample created. Sample shape: {sample.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf99be3c-f01d-4a68-8fdc-99ef0b0e5515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Saving final datasets with progress:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving files: 100%|█████████████████████████████| 2/2 [00:35<00:00, 17.95s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " All steps completed successfully.\n",
      "➡ Saved files:\n",
      "   - EDA_All_Recording_Traffic.parquet  (full cleaned dataset)\n",
      "   - EDA_All_Recording_Traffic.txt  (full dataset tab-delimited)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 6️) SAVE FINAL DATASETS\n",
    "# ============================================================\n",
    "\n",
    "from tqdm import tqdm\n",
    "import time  # just for simulating progress\n",
    "\n",
    "# List of files to save with descriptions\n",
    "save_tasks = [\n",
    "    #('EDA_All_Recording_Traffic_Sample.csv', lambda: sample.to_csv('EDA_Ready_All_Recording_Traffic_Sample.csv', index=False), 'Sample dataset (5%)'),\n",
    "    ('EDA_All_Recording_Traffic.parquet', lambda: traffic.to_parquet('EDA_Ready_All_Recording_Traffic.parquet', index=False), 'Full cleaned dataset'),\n",
    "    ('EDA_All_Recording_Traffic.txt', lambda: traffic.to_csv('EDA_Ready_All_Recording_Traffic.txt', sep='\\t', index=False), 'Full dataset tab-delimited')\n",
    "]\n",
    "\n",
    "print(\"\\n Saving final datasets with progress:\")\n",
    "\n",
    "for filename, func, desc in tqdm(save_tasks, desc=\"Saving files\", unit=\"file\"):\n",
    "    func()  # execute the save function\n",
    "    # Optional: tiny sleep to visually show progress if fast\n",
    "    time.sleep(0.1)\n",
    "\n",
    "print(\"\\n All steps completed successfully.\")\n",
    "print(\"➡ Saved files:\")\n",
    "#print(\"   - EDA_All_Recording_Traffic_Sample.csv  (for quick analysis)\")\n",
    "print(\"   - EDA_All_Recording_Traffic.parquet  (full cleaned dataset)\")\n",
    "print(\"   - EDA_All_Recording_Traffic.txt  (full dataset tab-delimited)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46027f1-8a6b-4621-be7c-9883ec4c5007",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "985016a8-1a6d-47f5-86e7-e6ecda87205d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows: 100%|███████████| 5383378/5383378 [00:01<00:00, 2742733.90it/s]\n",
      "Processing rows: 100%|████████████| 5383378/5383378 [00:16<00:00, 323950.17it/s]\n",
      "Processing rows: 100%|███████████| 5383378/5383378 [00:01<00:00, 3476634.44it/s]\n",
      "Processing rows: 100%|███████████| 5383378/5383378 [00:01<00:00, 3438012.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating stratified 5% sample (safe for all pandas versions)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows: 100%|█████████████████████| 1158/1158 [00:01<00:00, 618.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stratified sample created. Sample shape: (269176, 43)\n",
      "\n",
      "Feature engineering complete! Saved full dataset to: data/output/Traffic_PowerBI_Ready.csv\n",
      "Sample saved to: data/output/Traffic_PowerBI_Sample.csv\n"
     ]
    }
   ],
   "source": [
    "# Before running this script, perform exploratory data analysis to identify missing values\n",
    "# and any necessary type conversions or data cleansing.\n",
    "# The following code performs feature engineering to prepare the 'All Recording Traffic' dataset for Power BI:\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
    "\n",
    "tqdm.pandas(desc=\"Processing rows\")\n",
    "\n",
    "# 1. Load your traffic data from TXT\n",
    "df = pd.read_csv(\"EDA_Ready_All_Recording_Traffic.txt\", sep=\"\\t\")\n",
    "\n",
    "# 2. Convert DATE and DATETIME to datetime\n",
    "df[\"DATE\"] = pd.to_datetime(df[\"DATE\"])\n",
    "df[\"DATETIME\"] = pd.to_datetime(df[\"DATETIME\"])\n",
    "\n",
    "# 3. Extract Year and Month\n",
    "df[\"Year\"] = df[\"DATE\"].dt.year\n",
    "df[\"Month\"] = df[\"DATE\"].dt.month\n",
    "\n",
    "# 4. Is_Weekend flag\n",
    "df[\"Is_Weekend\"] = df[\"Day_Name\"].progress_apply(lambda d: 1 if d in [\"Saturday\",\"Sunday\"] else 0)\n",
    "\n",
    "# 5. Is_Holiday flag using pandas holiday calendar\n",
    "cal = USFederalHolidayCalendar()\n",
    "holidays = cal.holidays(start=df[\"DATE\"].min(), end=df[\"DATE\"].max())\n",
    "df[\"Is_Holiday\"] = df[\"DATE\"].progress_apply(lambda d: 1 if d in holidays else 0)\n",
    "\n",
    "# 6. Season\n",
    "def get_season(m):\n",
    "    if m in [12,1,2]:\n",
    "        return \"Winter\"\n",
    "    elif m in [3,4,5]:\n",
    "        return \"Spring\"\n",
    "    elif m in [6,7,8]:\n",
    "        return \"Summer\"\n",
    "    else:\n",
    "        return \"Fall\"\n",
    "df[\"Season\"] = df[\"Month\"].progress_apply(get_season)\n",
    "\n",
    "# 7. Violation_Rate (vectorized)\n",
    "df[\"Violation_Rate\"] = (df[\"VIOLATION\"] / df[\"TOTAL\"]).fillna(0)\n",
    "\n",
    "# 8. Week_of_Year\n",
    "df[\"Week_of_Year\"] = df[\"DATE\"].dt.isocalendar().week\n",
    "\n",
    "# 9. Hour and Time_of_Day\n",
    "df[\"Hour\"] = df[\"DATETIME\"].dt.hour\n",
    "def tod(h):\n",
    "    if 6 <= h < 12:\n",
    "        return \"Morning\"\n",
    "    elif 12 <= h < 18:\n",
    "        return \"Afternoon\"\n",
    "    elif 18 <= h < 22:\n",
    "        return \"Evening\"\n",
    "    else:\n",
    "        return \"Night\"\n",
    "df[\"Time_of_Day\"] = df[\"Hour\"].progress_apply(tod)\n",
    "\n",
    "# 10. Lagged and rolling features\n",
    "df = df.sort_values([\"FAC_B\",\"DATETIME\"])\n",
    "df[\"Total_Lag1D\"] = df.groupby(\"FAC_B\")[\"TOTAL\"].shift(24)\n",
    "df[\"Total_Rolling_3D\"] = (\n",
    "    df.groupby(\"FAC_B\")[\"TOTAL\"]\n",
    "      .transform(lambda x: x.rolling(window=72, min_periods=1).mean())\n",
    ")\n",
    "\n",
    "# 12. STRATIFIED SAMPLING (5%)\n",
    "df[\"month_year\"] = df[\"DATE\"].dt.to_period(\"M\")\n",
    "print(\"\\nCreating stratified 5% sample (safe for all pandas versions)...\")\n",
    "sample = (\n",
    "    df.groupby(['FAC_B', 'month_year'], group_keys=False)\n",
    "      .progress_apply(lambda x: x.sample(frac=0.05, random_state=1) if len(x) > 1 else x)\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "print(f\"Stratified sample created. Sample shape: {sample.shape}\")\n",
    "\n",
    "\n",
    "# Save full dataset and sample\n",
    "output_file = \"data/output/Traffic_PowerBI_Ready.csv\"\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "\n",
    "df.to_csv(output_file, index=False)\n",
    "print(f\"\\nFeature engineering complete! Saved full dataset to: {output_file}\")\n",
    "\n",
    "sample_file = \"data/output/Traffic_PowerBI_Sample.csv\"\n",
    "sample.to_csv(sample_file, index=False)\n",
    "print(f\"Sample saved to: {sample_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f274b21e-aece-46d2-8c56-75f1dfc5d602",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n"
     ]
    }
   ],
   "source": [
    "# Note: This script calls / executes another Python script/notebook \n",
    "# ('data_ingestion_facility_speeds.ipynb' or its converted .py version) \n",
    "# to ensure the required data file 'Facility_Mobility_Speeds_Clean.csv' is generated beforehand.\n",
    "\n",
    "#import nbformat\n",
    "#from nbconvert.preprocessors import ExecutePreprocessor\n",
    "\n",
    "#def run_notebook(path):\n",
    "#    with open(path) as f:\n",
    "#        nb = nbformat.read(f, as_version=4)\n",
    "#    ep = ExecutePreprocessor(timeout=600, kernel_name='python3')\n",
    "#    ep.preprocess(nb, {'metadata': {'path': './'}})  # Change path if needed\n",
    "\n",
    "#run_notebook('data_ingestion_facility_speeds.ipynb')\n",
    "\n",
    "\n",
    "# Facility Mobility Speeds file cleaning. Removing extra columns\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 1. Load the TXT file (tab-separated)\n",
    "df = pd.read_csv(\"Facility Mobility Speeds.txt\", sep=\"\\t\")\n",
    "\n",
    "# 2. Drop the static Facility_Order column\n",
    "df = df.drop(columns=[\"Facility_Order\"])\n",
    "\n",
    "# 3. Format Month_Year with a progress bar\n",
    "tqdm.pandas(desc=\"Formatting Month_Year\")\n",
    "df[\"Month_Year\"] = df[\"Month_Year\"].progress_apply(lambda x: pd.to_datetime(x).strftime(\"%Y-%m\"))\n",
    "\n",
    "# 4. Save the cleaned data for use in Power BI or further analysis\n",
    "output_file = \"Facility_Mobility_Speeds_Clean.csv\"\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "# 5. Display a summary message\n",
    "print(f\"\\nFinal cleaned data ({df.shape[0]} records, {df.shape[1]} columns) saved to: {output_file}\")\n",
    "\n",
    "# Facility mobility speeds - Feature Engineering for Predictive Analysis (if required)\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load cleaned data\n",
    "df = pd.read_csv(\"Facility_Mobility_Speeds_Clean.csv\")\n",
    "\n",
    "# Feature engineering with progress bar\n",
    "tqdm.pandas(desc=\"Feature Engineering\")\n",
    "\n",
    "# 1. Extract year and month from Month_Year\n",
    "df[\"Month_Year\"] = pd.to_datetime(df[\"Month_Year\"], format=\"%Y-%m\")\n",
    "df[\"Year\"] = df[\"Month_Year\"].dt.year\n",
    "df[\"Month\"] = df[\"Month_Year\"].dt.month\n",
    "\n",
    "# 2. One-hot encode Direction\n",
    "df = pd.get_dummies(df, columns=[\"Direction\"], prefix=\"Dir\")\n",
    "\n",
    "# 3. (Optional) Create lag features for Avg_Speed (previous month per facility)\n",
    "df = df.sort_values([\"Facility\", \"Month_Year\"])\n",
    "df[\"Avg_Speed_Lag1\"] = df.groupby(\"Facility\")[\"Avg_Speed\"].shift(1)\n",
    "\n",
    "# 4. (Optional) Standardize continuous features\n",
    "#for col in [\"Freeflow\", \"Avg_Speed\", \"Delta\"]:\n",
    "#    mean = df[col].mean()\n",
    "#    std = df[col].std()\n",
    "#    df[f\"{col}_z\"] = (df[col] - mean) / std\n",
    "\n",
    "# 5. Save engineered features\n",
    "output_file = \"Facility_Mobility_Speeds_Features.csv\"\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"\\nFeature engineered data ({df.shape[0]} records, {df.shape[1]} columns) saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d0cf6fe4-d554-4c36-a70e-49958ca77822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traffic_PowerBI_Ready.csv loaded successfully.\n",
      "Facility_Mobility_Speeds_Clean.csv loaded successfully.\n",
      "\n",
      "Processing Traffic Data...\n",
      "Traffic data aggregated and has 1158 monthly records.\n",
      "\n",
      "Processing Mobility Speed Data...\n",
      "Facility short codes replaced with full names.\n",
      "\n",
      "Merging DataFrames...\n",
      "\n",
      "--- Final Merged DataFrame Head ---\n",
      "  Facility_Name Month_Year  Aggregated_Traffic_Volume  Freeflow  Avg_Speed  \\\n",
      "0       Bayonne    2013-01                   279932.0       0.0        0.0   \n",
      "1       Bayonne    2013-02                   249747.0       0.0        0.0   \n",
      "2       Bayonne    2013-03                   283081.0       0.0        0.0   \n",
      "3       Bayonne    2013-04                   288071.0       0.0        0.0   \n",
      "4       Bayonne    2013-05                   302573.0       0.0        0.0   \n",
      "\n",
      "  Direction  Delta  \n",
      "0         0    0.0  \n",
      "1         0    0.0  \n",
      "2         0    0.0  \n",
      "3         0    0.0  \n",
      "4         0    0.0  \n",
      "\n",
      "Shape of the final merged DataFrame: (1334, 7)\n",
      "\n",
      "Data successfully saved to Merged_Traffic_Mobility_Data_Full.csv\n",
      "\n",
      "Filtering data to keep only rows where Aggregated_Traffic_Volume and Freeflow are non-zero...\n",
      "\n",
      "--- Saving Filtered Merged DataFrame ---\n",
      "    Facility_Name Month_Year  Aggregated_Traffic_Volume  Freeflow  Avg_Speed  \\\n",
      "313     GWB Lower    2024-01                  1685357.0     45.90       34.5   \n",
      "314     GWB Lower    2024-01                  1685357.0     46.05       25.6   \n",
      "315     GWB Lower    2024-02                  1673098.0     45.90       32.2   \n",
      "316     GWB Lower    2024-02                  1673098.0     46.10       22.5   \n",
      "317     GWB Lower    2024-03                  1877547.0     45.90       32.4   \n",
      "\n",
      "    Direction  Delta  \n",
      "313        WB  -11.4  \n",
      "314        EB  -20.5  \n",
      "315        WB  -13.7  \n",
      "316        EB  -23.6  \n",
      "317        WB  -13.5  \n",
      "Shape of the final merged and filtered DataFrame: (32, 7)\n",
      "Data successfully saved to Merged_Traffic_Mobility_Data_FILTERED.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Data Pipeline: Traffic and Mobility Data Merge and Aggregation\n",
    "\n",
    "PURPOSE:\n",
    "Aggregates hourly traffic data ('Traffic_PowerBI_Ready.csv') to a monthly level,\n",
    "harmonizes facility names with mobility speed data ('Facility_Mobility_Speeds_Clean.csv'), \n",
    "performs an outer merge, and saves two distinct output files.\n",
    "\n",
    "INPUTS:\n",
    "1. 'Traffic_PowerBI_Ready.csv': Hourly traffic counts, requires aggregation by Month/Facility.\n",
    "2. 'Facility_Mobility_Speeds_Clean.csv': Already aggregated monthly speed data using short facility codes.\n",
    "    ** Note: To generate this file, run data_ingestion_facility_speeds.ipynb beforehand.\n",
    "\n",
    "OUTPUTS:\n",
    "1. 'Merged_Traffic_Mobility_Data_FULL.csv': \n",
    "    - Contains all combinations of Facility and Month from both input files (Outer Join).\n",
    "    - Missing metric values (where data existed in one file but not the other) are filled with 0.\n",
    "2. 'Merged_Traffic_Mobility_Data_FILTERED.csv':\n",
    "    - Filtered version of the FULL dataset.\n",
    "    - Only includes rows where both 'Aggregated_Traffic_Volume' (from Traffic) AND 'Freeflow' \n",
    "      (the primary speed metric) are non-zero. Use this file for direct comparison analysis.\n",
    "\n",
    "ASSUMPTIONS & NOTES:\n",
    "- The 'Traffic_PowerBI_Ready.csv' file uses the column 'TOTAL' for traffic volume.\n",
    "- The traffic data contains historical records (e.g., 2013-2025), while the speed data \n",
    "  is currently limited (e.g., 2024-2025). The FULL merge preserves all historical traffic \n",
    "  data alongside available speed data.\n",
    "- Facility names are mapped using the SHORT_TO_LONG_MAPPING defined below.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. Define Mapping ---\n",
    "# Mapping from Facility short code (in Facility_Mobility_Speeds_Clean) \n",
    "# to full name (in Traffic_PowerBI_Ready)\n",
    "SHORT_TO_LONG_MAPPING = {\n",
    "    \"BB\": \"Bayonne Bridge\",\n",
    "    \"GB\": \"Goethals Bridge\",\n",
    "    # The GWB short code maps to 'GWB Lower' for consistency when merging with the \n",
    "    # granular traffic data which may contain 'GWB Lower' and 'GWB PIP'.\n",
    "    \"GWB\": \"GWB Lower\", \n",
    "    \"HT\": \"Holland Tunnel\",\n",
    "    \"LT\": \"Lincoln Tunnel\",\n",
    "    \"OBX\": \"Outerbridge Crossing\"\n",
    "}\n",
    "\n",
    "# --- 2. Load Data ---\n",
    "\n",
    "# Load Traffic_PowerBI_Ready.csv (hourly data with FAC_B and 'TOTAL' column)\n",
    "df_traffic = pd.read_csv('data/output/Traffic_PowerBI_Ready.csv') \n",
    "print(\"Traffic_PowerBI_Ready.csv loaded successfully.\")\n",
    "\n",
    "# DEBUGGING STEP: Removed after identifying the issue (TOTAL vs Total)\n",
    "\n",
    "# Load Facility_Mobility_Speeds_Clean.csv (aggregated speed data with 'Facility' short code)\n",
    "df_speeds = pd.read_csv(\"Facility_Mobility_Speeds_Clean.csv\")\n",
    "print(\"Facility_Mobility_Speeds_Clean.csv loaded successfully.\")\n",
    "\n",
    "\n",
    "# --- 3. Process Traffic_PowerBI_Ready (Aggregation) ---\n",
    "print(\"\\nProcessing Traffic Data...\")\n",
    "\n",
    "# Convert the time column ('month_year') to a monthly period for aggregation\n",
    "df_traffic['Month_Year'] = pd.to_datetime(df_traffic['month_year']).dt.to_period('M')\n",
    "\n",
    "# Aggregate the hourly data by FAC_B and Month_Year, summing the CORRECT 'TOTAL' column.\n",
    "df_traffic_agg = df_traffic.groupby(['FAC_B', 'Month_Year']).agg(\n",
    "    Aggregated_Traffic_Volume=('TOTAL', 'sum')  # <-- CORRECTED: Changed 'Total' to 'TOTAL'\n",
    ").reset_index()\n",
    "\n",
    "# Rename the facility column to the common merging key\n",
    "df_traffic_agg.rename(columns={'FAC_B': 'Facility_Name'}, inplace=True)\n",
    "print(f\"Traffic data aggregated and has {len(df_traffic_agg)} monthly records.\")\n",
    "\n",
    "\n",
    "# --- 4. Process Facility_Mobility_Speeds_Clean (Mapping) ---\n",
    "print(\"\\nProcessing Mobility Speed Data...\")\n",
    "\n",
    "# Convert the time column ('Month_Year') to a monthly period for merging\n",
    "df_speeds['Month_Year'] = pd.to_datetime(df_speeds['Month_Year']).dt.to_period('M')\n",
    "\n",
    "# Map the short Facility codes (BB, GB) to the full names \n",
    "df_speeds['Facility_Name'] = df_speeds['Facility'].replace(SHORT_TO_LONG_MAPPING)\n",
    "\n",
    "# Drop the old short code column\n",
    "df_speeds.drop(columns=['Facility'], inplace=True)\n",
    "print(\"Facility short codes replaced with full names.\")\n",
    "\n",
    "\n",
    "# --- 5. Merge Data and Fill NaN ---\n",
    "print(\"\\nMerging DataFrames...\")\n",
    "\n",
    "# Perform an 'outer' merge on the common keys ['Facility_Name', 'Month_Year']\n",
    "merged_df = pd.merge(\n",
    "    df_traffic_agg, \n",
    "    df_speeds, \n",
    "    on=['Facility_Name', 'Month_Year'], \n",
    "    how='outer'\n",
    ")\n",
    "\n",
    "# Identify the columns that need to be zero-filled (the numeric metric columns)\n",
    "key_columns = ['Facility_Name', 'Month_Year']\n",
    "metric_columns = [col for col in merged_df.columns if col not in key_columns]\n",
    "\n",
    "# Fill non-matching metric values (NaNs from the outer join) with 0\n",
    "merged_df[metric_columns] = merged_df[metric_columns].fillna(0)\n",
    "\n",
    "# Convert Month_Year back to a standard string format for final output\n",
    "merged_df['Month_Year'] = merged_df['Month_Year'].astype(str)\n",
    "\n",
    "print(\"\\n--- Final Merged DataFrame Head ---\")\n",
    "print(merged_df.head())\n",
    "print(f\"\\nShape of the final merged DataFrame: {merged_df.shape}\")\n",
    "\n",
    "# --- 6. Save the Merged Data ---\n",
    "merged_df.to_csv('data/output/Merged_Traffic_Mobility_Data_Full.csv', index=False)\n",
    "print(\"\\nData successfully saved to Merged_Traffic_Mobility_Data_Full.csv\")\n",
    "\n",
    "\n",
    "# --- 6. Filter for Non-Zero Metrics (Create the FILTERED Dataframe) ---\n",
    "# Filter to keep only rows where the main traffic metric AND \n",
    "# the primary speed metric (Freeflow) are non-zero.\n",
    "# We create a new DataFrame 'filtered_df' for this specific use case.\n",
    "\n",
    "filtered_df = merged_df.copy()\n",
    "\n",
    "if 'Freeflow' in filtered_df.columns:\n",
    "    print(\"\\nFiltering data to keep only rows where Aggregated_Traffic_Volume and Freeflow are non-zero...\")\n",
    "    filtered_df = filtered_df[\n",
    "        (filtered_df['Aggregated_Traffic_Volume'] != 0) & \n",
    "        (filtered_df['Freeflow'] != 0)\n",
    "    ].copy()\n",
    "else:\n",
    "    print(\"\\nWarning: 'Freeflow' column not found for non-zero filtering. Skipping non-zero filter for filtered file.\")\n",
    "\n",
    "# Convert Month_Year back to a standard string format for final output (FILTERED data)\n",
    "filtered_df['Month_Year'] = filtered_df['Month_Year'].astype(str)\n",
    "\n",
    "print(\"\\n--- Saving Filtered Merged DataFrame ---\")\n",
    "print(filtered_df.head())\n",
    "print(f\"Shape of the final merged and filtered DataFrame: {filtered_df.shape}\")\n",
    "\n",
    "# --- 7. Save the Filtered Data ---\n",
    "filtered_df.to_csv('data/output/Merged_Traffic_Mobility_Data_FILTERED.csv', index=False)\n",
    "print(\"Data successfully saved to Merged_Traffic_Mobility_Data_FILTERED.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73261ee6-53fb-4918-8adc-9c07c83b8b3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
